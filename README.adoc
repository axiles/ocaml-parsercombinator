// vim:filetype=asciidoc expandtab spell spelllang=en ts=2 sw=2
= Resumable Parser Combinators with error detection
rixed@happyleptic.org
v0.1, 2015-10-27
:toc:
:numbered:
:icons:
:lang: en
:encoding: utf-8

== What is this?

This document describe the implementation of a library to perform text
parsing according to a technique called
https://en.wikipedia.org/wiki/Parser_combinator[parser combinators] because the
parser for a given grammar is build combining simpler parsers. This technique
is fashionable although old and slow because it is simple and permissive.

This is also an example of literate programming; in case the notations used in
this document are unclear you can read about them in
http://rixed.github.io/portia/notations.html[this document].

== Requirements

=== A N+1^th^ parser to rule them all

Along the years I've often wrote small dedicated parsing libraries, either using
lex+yacc approach for well behaved syntaxes or custom made combinatoric parsers
because they are fun to play with and more flexible. My needs ranged from
parsing configuration files, parsing programming languages, parsing natural
languages, parsing networking protocols...

This parser is meant to be flexible enough to accommodate all those needs, when
speed is not an issue.

=== Parsing binary

Most of the time one need to parse text, but some time the need arise to parse
binary messages (such as when parsing some networking protocols). Therefore the
first requirement is to not be bound to text. Therefore parsed input tokens
must be of any type α.

=== Parsing recursively

I want to parse as much as possible from perl and other weird scripting
languages (without executing them).  Perl has +eval+ on strings. Therefore
parsing must not stop as soon as we reach a string literal but instead recurse
and try to parse that string. It must therefore be possible to write such a
recursive set of rules.

=== Parsing with gaps

For the same reason we want to be able to parse as much as possible of an input
even if we are missing some parts of it. This is useful both to parse string
fragments into code  ut also to parse a networking stream when some packets are
missing.

=== Best effort

It will not always be possible to know for sure the type of all symbols. In
that case we want to present all possible alternatives as the result, rather
than stopping the parse or selecting the most probable one.

=== Resumable parsers

We want the parser to be usable for parsing large messages either received from
the network or stored on a file, without having to buffer everything
(especially since, as we will see, we will have more to store per input
character than the mere character). Input tokens must therefore be consumed as
they arrive, and the parser must "return" when it has done as much parsing as
possible on the given input, and be resumable later on when more data is
available.

For this, +call_cc+ would help tremendously to freeze the state of affairs and
return it to the original caller.  Without that facility, though, alternatives
do exist:

A conventional method to achieve something similar is to turn our API (aka our
types) inside out: instead of taking a stream of input tokens and returning a
result, parsers could take the next item and return, along with each result, a
next-step parser to apply to the next token. This is rather inconvenient,
though.

Alternatively, threading is another way to freeze ones stack: a parser waiting
for input could just be waiting for a conditional variable, or the input stream
could be a `mailbox variable` kind of queue, etc. All this amount to relying on
the kernel to freeze the parser and resume it later.

We might want to use those parsers in an environment where using POSIX threads
is not an option, though (such as: in a microkernel).  In that case lightweight
threads (akin to gnupth) could work, but that's still a huge constraint on the
environment. Instead, cooperative threads (also sometime referred to as
"lightweight threads") are cheap and depends on nothing ; for this reason they
are available in many shapes and forms. The downside is that the API needs to
accommodate for those (basically, each parser returns a future value rather
than a value). But once this abstraction is in place then one is free to use it
for cooperative threads or POSIX threads or make it a bypass. That's thus what
this library will be using, at the cost of a bit of functorization.

=== Error detection?

The input that will be sent to this parser will already be valid most of the
time.  Then, error reports must _only_ include the (best) partial result(s) and
why they failed.  It is very unfortunate though that even in those favorable
circumstances detecting what is the best attempt in order to explain why it
failed seems to require the same expensive error detection techniques than to
detect user typos.

Indeed, the only way I could think of to unearth the best attempt is: "had the
parsing of this bit behaved differently then the whole outcome would have been
much better".  In other words: "had this parsing step accepted this input as
valid, we would eventually have parsed the whole input".  And this requires to
try different behavior at every step.

It is hard to think of any trivial trick to make it less awfully expensive than
it sounds, beyond constraining the exploratory search within an 'error budget'.

Many of those ideas come from
http://www.staff.science.uu.nl/~swier101/Papers/1999/SofSem99.pdf[Fast, Error
Correcting Parser Combinators: A Short Tutorial], a XX^th^ century paper.

The good side is: we can turn off the feature and reclaim most of its cost by
making the error budget zero.  And if detecting typos or other errors is a
thing we have some level of control of the time spent on it.

=== Parser Combinators

Scripting languages in general, and perl in particular, do not bow to any
rigid formal grammar. Parser combinators are appealing because they make it
possible to add new valid constructs without rethinking the whole grammar; in
other words, to build a good enough parser iteratively.  In addition, they
make it significantly easier to achieve the previous objectives.

== The type of the parser

To be clearer, let's write down the usual type of a parser used with
combinators, written in ML:

// TODO: a better name than Temp
.Temp: Typical parser type
[source,ml]
----
type ('a, 'b) result = 'b * 'a list
type ('a, 'b) parser = 'a list -> ('a, 'b) result list
----

Which reads like this: Calling α the type of the input tokens and β the type
of the parsed inputs, a parser is a function that takes a list of α and
returns a list of pairs composed of a β and a list of α, with the assumptions
that the input list of α is the input stream of tokens to be parsed and that
each output pair are a possible solution, composed of the result and the
remaining list of token to parse. Ideally, a successful top level parser will
thus return a list composed of a single pair (non ambiguity of the outcome)
made of the final result and an empty list (no more input to be parsed). And
when the parser fails to find any way to parse the input it returns a
minimally informative empty list.

For resumable parsing in a possibly cooperative threading context we need to
introduce the +'a ct+ type (for a +cooperative thread+ of some form returning
an +'a+), and make the input stream of tokens a possibly blocking function
returning the next item, also turning our Parsers module into a functor
depending on what mechanism we plan to use for cooperation between parsers
and token input:

.Parsers.ml: functor to parametrize over the cooperation mechanism
[source,ml]
----
module type CONFIG = sig
  (* ...Parser configuration... *)
end

module type S = sig
  include CONFIG

  (* ...Parser signatures... *)
end

module Make (Conf : CONFIG) :
  S with type 'a ct = 'a Conf.ct
     and type 'a stream = 'a Conf.stream =
struct
  include Conf

  (* ...Parser library... *)
end
----

Due to functorization we have to explicitly provide a signature for the
result of +Make+ so that we can use the resulting parser as input of further
functors. I regard this as one of the major annoyance of OCaml but hopefully
it will help us check our understanding of what we are going to define.

The configuration must thus provides the actual type for frozen computation
(aka cooperative thread) as well as the possibly blocking input mechanism:

.Parser configuration
[source,ml]
----
type 'a ct
type 'a stream
val take : 'a stream -> ('a Misc.stream_value * 'a stream) ct
----

With the +stream_value+ type defined in a separate module that can therefore be
included from all around the place without fearing circular dependencies:

.Misc.ml: type of stream value
[source,ml]
----
type 'a stream_value = Item of 'a | EndOfStream
----

Notice that this +stream+ container must be free of side effects to the extend
that any token read from it in one place must still be available for reading
from previously stored streams. That is why +take+ returns both the
next token and the next (shorter) stream.

We also need a way to wrap a value into a `cooperative thread` returning that value,
and a way to pipe one `thread` into another, both operations typically called
`return` and `bind` but here prefixed with +ct_+ before we reserve those names
for our parser combinators:

.Parser configuration
[source,ml]
----
val ct_return : 'a -> 'a ct
val ct_bind : 'a ct -> ('a -> 'b ct) -> 'b ct
----

Thus a parser now has this shape:

.Temp: Resumable parser type
[source,ml]
----
type ('a, 'b) result = 'b * 'a stream
type ('a, 'b) parser = 'a stream -> ('a, 'b) result list ct
----

It will come handy to have a dummy configuration with no fancy threading in
place, for those cases where we do not care for the parsers to be resumable or
for testing. In this case out stream could be a mere list.

The +SimpleConfig+ serves this purpose:

.Parsers.ml: simple configuration for non-resumable parsers
[source,ml]
----
open Misc
module SimpleConfig =
struct
  type 'a ct = 'a
  type 'a stream = 'a list
  let take = function
    | [] -> EndOfStream, []
    | x :: rest -> Item x, rest
  let ct_return x = x
  let ct_bind x f = f x
  (* ...other SimpleConfig definitions... *)
end
----

Introducing the error budget changes this somewhat: we will try to
artificially force the failing parsers to succeed in order to peek into the
future and try to locate where a change would cause the parsing to succeed.
This means that each individual result must be accompanied by a description of
the (few) changes required to reach that point:

.Temp: Parser type with error budget
[source,ml]
----
type ('a, 'b) result = 'b * Corrections.t * 'a stream
type ('a, 'b) parser =
  Corrections.t -> 'a stream -> ('a, 'b) result list ct
----

Before looking closer at this new +Corrections.t+ type that would encode the
corrections we must question the usage of a list of results as the return type.
Firstly, a list is over-specified since the order of the possible results is
not important; what we really want here is a set and we use a list only because
it makes our code more terse. Secondly, and more importantly, an empty list to
signal failure seems not enough if we want to artificially force the _failure_
of parsers.

----
class form;
for x = new form(...);
----

Obviously the intent was to write +form x = ...+. Imagine the rule to parse the
second line is +is_keyword XOR is_name+. Once the parser have accepted the
keyword +for+ as valid without questioning it then it is likely that the error
message porting on what follows will be hard to understand. On another hand, if
the parser also tried to force the failure of the keyword parser in this
location then it will notice that everything would parse properly henceforth,
suggesting a better error message. So it seems beneficial to return that
failure as a correction and move on to next token.

The price to pay for testing the failure of successful parsers is obviously high,
though, and not only because of the additional time spent. Returning error
descriptions alongside failures forces us to give up the elegant list of
result as the main return type (indeed, the empty list is not enough any more
to denote failure since we also want to return the updated errors description).

But it seems that only in the case of the exclusive alternative this problem
will arise. Should we decide not to implement such a combinator, then the above
example ``either a keyword or a variable name that is not a keyword'' could
still be written with inclusive alternative at the price of a redundant check:
+is_keyword OR (is_name AND (check (NOT is_keyword))+. In this case we could
explore the failure of the +NOT is_keyword+ check and will find that should
+for+ be a valid variable name then the input would be valid, which will make a
good enough error reporting.

So we will not implement exclusive alternative and will instead explore forced
success of the +check+ parser. Hence we saved our list of results as the return
type.

Now, what's this +Corrections.t+ type?

To be able to build a useful error message we must point at the position in
the original stream of tokens where some change had to be made in order to
parse the input stream of tokens (if not in full at least more than without
that change). Recording a position in the original input stream of tokens is
less trivial than it sounds because we are combining the parsers, and one of
the way to combine parsers is to run a parser on the results of another one,
therefore loosing track of the position in the original stream. For generality
let's introduce a new type ɣ to denote a position (could be merely the integer
offset in the original stream or a more elaborate line and column number, or
anything) and let's assume we read from the original stream not only the
tokens but also the positions.

So we need to record all changes that have already occurred (what and where)
and how many changes we are still allowed to do. The ``What'' is limited to
any string description of the parser that we forced to succeed. For the
position we conveniently reuse the stream output as it can already encode both
the position and the special +EOF+ position:

.Corrections.ml: type
[source,ml]
----
open Misc
type ('c, 'a) t = (('a * 'c) stream_value * string) BoundedSet.t
----

where +BoundedSet+ is an unordered container with a maximum capacity (the
maximum amount of changes allowed) and which API will become clearer as we
encounter the few required functions.

Trivially, to add an error at position +pos+ to the correction list, with
message +msg+:

.Corrections.ml: recording a change
[source,ml]
----
let change_at corr tokpos (msg : string) =
  BoundedSet.add corr (tokpos, msg)
----

Now that we know what corrections look like and that we have to read the
positions alongside the tokens from the input stream, we can finally write the
definitive parser type:

.final parser type
[source,ml]
----
type ('a, 'b, 'c) result = 'b * (('c, 'a) Corrections.t) * ('a * 'c) stream
type ('a, 'b, 'c) t =
  ('c, 'a) Corrections.t -> ('a * 'c) stream -> ('a, 'b, 'c) result list ct
----

.Parser library
[source,ml]
----
(* ...final parser type... *)
----

.Parser signatures
[source,ml]
----
(* ...final parser type... *)
----

It is sometime unfortunate that OCaml compiler do not preserve the type
variable names throughout a program. In this document though we will stick
with those:

- α (+'a+) stands for the type of input tokens,
- β (+'b+) stands for the type of parsing results,
- ɣ (+'c+) stands for the type of positions in the input stream.

== Base parsers (to be combined)

=== Fail, Return and check.

The simplest parsers that does nothing are +return+ and +fail+. They do not
consume anything from the input but merely return a single result or no result
at all. Given our parser type, here are their implementations:

.Parser library: fail
[source,ml]
----
let fail _corr _stream = ct_return []
----

.Parser library: return
[source,ml]
----
let return x corr rest = ct_return [x, corr, rest]
----

with signatures:

.Parser signatures
[source,ml]
----
val fail : ('a, 'b, 'c) t
val return : 'b -> ('a, 'b, 'c) t
----

Those two first parsers perform no error detection at all.  But many other
parsers will have to either terminate parsing abruptly (with +fail+) or add a
change to the correction list and proceed, if the error budget is not exhausted
already. We will abstract this in a +fail_or_maybe_not+ function:

.Parser library: fail with success exploration
[source,ml]
----
open Batteries
open Corrections

let fail_or_maybe_not msg x ?pos_from corr rest =
  ct_bind
    (take (Option.default rest pos_from))
    (fun (tokpos, _) ->
      if BoundedSet.is_full corr then (
        (* no more errors permitted so fail for real *)
        ct_return []
      ) else (
        ct_return [x, change_at corr tokpos msg, rest]))
----

Notice that the +pos_from+ parameter allows the caller to set the position in
addition to the message of the correction. By default the position will be
taken from the passed stream (+rest+).

Another parser that does not consume any input is the +check+ parser that we
have mentioned earlier. It is actually a combinator since it takes another
parser as parameter. It checks that the given parser succeed but then return
the input stream unchanged (with a +unit+ result). The only thing interesting
is that it explores forcing a success in case the check fails.

.Parser library: check
[source,ml]
----
let check msg p corr rest =
  ct_bind (p corr rest) (function
    | [] -> fail_or_maybe_not msg () corr rest
    | _ -> return () corr rest)
----

.Parser signatures
[source,ml]
----
val check : string -> ('a, 'b, 'c) t -> ('a, unit, 'c) t
----

Another parser that will prove useful (despite contributing no value to the
result) especially in coordination with +check+ is the negation:

.Parser library: negation
[source,ml]
----
let nay p corr rest =
  ct_bind (p corr rest) (function
    | [] -> return () corr rest
    | _ -> fail_or_maybe_not "not" () corr rest)
----

So that we could write +check msg (nay p)+.

.Parser signatures
[source,ml]
----
val nay : ('a, 'b, 'c) t -> ('a, unit, 'c) t
----

=== Tests

It is important to have a test infrastructure in place before it's needed.
Given literate programing allows us to mix code and tests at ease we do not
need to get this feature from such a tool as
https://github.com/vincent-hugot/iTeML[qtest] and will use
http://ounit.forge.ocamlcore.org/api-ounit/index.html[oUnit] directly.

Supposing for now that we have all the required printers we can set up a
satisfying environment for tests:

.test.ml: the stage.
[source,ml]
----
open Batteries
open OUnit2
open Corrections
open Misc

module P = Parsers.Make (Parsers.SimpleConfig)
(* ...other tested modules... *)
open P

let stream_of_string s =
  let rec loop n tl =
    if n < 0 then tl else
    loop (n-1) ((s.[n], n) :: tl) in
  loop (String.length s - 1) []

let max_changes = 3
let corr = BoundedSet.make max_changes
let correction_at pos msg =
  let corr = BoundedSet.make max_changes in
  change_at corr pos msg

let no_corr = BoundedSet.make 0
let rest = stream_of_string "glop glop pas glop"
let no_input = []

(* ...other global functions or types for testing... *)

let uniq = function
  | [x, _, _] -> Some x
  | _ -> None

(* version of assert_equal specialized for parser results *)
let assert_same_results ?msg print_output exp actual =
  let print_coord fmt (c, p) =
    Printf.fprintf fmt "offset %d at \"%c...\"" p c in
  let result_printer = print_result print_coord print_output in
  let results_printer = List.print result_printer in
  ct_bind actual (fun got ->
    (* OUnit really should have a assert_same_elements *)
    ct_return (
      assert_equal ~printer:(IO.to_string results_printer) ?msg
        (exp |> List.sort compare)
        (got |> List.sort compare))) |>
  ignore

let test_suite =
  "test helpers" >:::
    [ "stream_of_empty" >:: (
        fun _ctx ->
          assert_equal ~printer:string_of_int
            0 (List.length (stream_of_string ""))) ;
      "stream_of_string basic" >:: (
        fun _ctx ->
          assert_equal ~printer:string_of_int
            2 (List.length (stream_of_string "ab"))) ;
      ] ;
  "tests" >:::
    [ (* ...tests... *) ]

let () =
  run_test_tt_main test_suite
----

Notice that we have to force the type of +assert_same_results+ to be +unit+
(with +ignore+) otherwise it would be +unit P.ct P.ct+, which should be
demonstrably equivalent to +unit+ given +SimpleConfig+, but for some reason the
compiler still nag about it.

That we can test with simple tests for +return+ and +fail+ (which really
does not cause too much worries):

.tests
[source,ml]
----
"return succeed" >:: (
  fun _ctx ->
    assert_same_results Int.print
      [42, no_corr, rest]
      (return 42 no_corr rest)
) ;
"return succeed even at EOF" >:: (
  fun _ctx ->
    assert_same_results Int.print
      [42, no_corr, []]
      (return 42 no_corr no_input)
) ;
"fail fails" >:: (
  fun _ctx ->
    assert_same_results Int.print
      []
      (fail no_corr rest)
) ;
"fail fails even at EOF" >:: (
  fun _ctx ->
    assert_same_results Int.print
      []
      (fail no_corr no_input)
) ;
----

=== Checking for end of stream

Another very useful and basic parser is the one that succeeds on EOF and fails
everywhere else. It is useful to check that the input stream have been consumed
entirely by the preceding parsers).

We may not want tp engage in error detection in this parser: mimicking success
imply pretending the stream stops there, but most input streams could be
trivially declared valid if the stream is cut short (empty string is often
valid for instance). In case of spurious input tokens at the end the error
message shouldn't the error message be trivial enough already? That's
forgetting that we are going to combine parsers, and ``end of stream'' does not
necessarily mean ``end of outer input stream''. It could for instance means the
``end of lines'' in a message header or the ``end of initializers'' in an
initializer list.

We will therefore explore forcing the success of this parser, which implies
cutting the unparsed stream when mimicking success:

.Parser library: checking for EOF
[source,ml]
----
open Misc
let eof msg corr rest =
  ct_bind (take rest) (function
    | EndOfStream, rest' -> return () corr rest'
    | _ ->
      fail_or_maybe_not
        ("spurious "^msg) () 
        ~pos_from:rest corr empty_stream (* <1> *))
----

<1>: Here we will restart with +rest = []+. That's the only case when forcing
success also alters the input stream.

.Parser signatures
[source,ml]
----
val eof : string -> ('a, unit, 'c) t
----

This +empty_stream+ must therefore be provided by the parser configuration:

.Parser configuration: empty stream
[source,ml]
----
val empty_stream : 'a stream
----

.other SimpleConfig definitions
[source,ml]
----
let empty_stream = []
----

And the accompanying test:

.tests
[source,ml]
----
"eof succeed" >:: (
  fun _ctx ->
    assert_same_results Unit.print
      [(), no_corr, []]
      (eof "char" no_corr no_input)
) ;
"eof fails" >:: (
  fun _ctx ->
    assert_same_results Unit.print
      []
      (eof "char" no_corr rest)
) ;
"eof suggests truncation" >:: (
  fun _ctx ->
    assert_same_results Unit.print
      [(), correction_at (Item ('g', 0)) "spurious char", []]
      (eof "char" corr rest)
) ;
----

=== First non trivial parser

The more general of parsers that do consume some input is the +cond+ parser,
which tries to recognize a condition on the next token (for instance that it
is equal to a given value). So +cond+ is a function that takes a predicate on
token and returns a parser that, when given this token, returns it (and
consumes it), or otherwise fails (with a message describing what it was
looking for, in case we have to build an error message for that step later
on).

Now that we know the type, writing the code is rather easy:

.Temp: the cond parser
[source,ml]
----
let cond msg f x corr rest =
  ct_bind (take rest) (function
    | EndOfStream, rest' ->
      if BoundedSet.is_full corr then
        ct_return []
      else
        ct_return [ x (* <1> *), change_at corr EndOfStream msg, rest' ]
    | Item (tok, _pos), rest' when f tok ->
      ct_return [ tok, corr, rest' ]
    | item, rest' ->
      if BoundedSet.is_full corr then
        ct_return []
      else
        ct_return [ x, change_at corr item  msg, rest' ])
----

So for each possible case (+EOF+, +f+ succeeds or +f+ fails) we prepare both
the result and the altered result and return both whenever possible.

<1> Here we need an example value +x+ of type β in order to change
the outcome of a failure at end of input. Which value exactly is not really a
concern since only its type matters (although the error message could print it
as an example, as OCaml compiler does when complaining about an incomplete
pattern matching.

.Parser signatures
[source,ml]
----
val cond : string -> ('a -> bool) -> 'a -> ('a, 'a, 'c) t
----

+cond_map+ is a +cond+ that returns an optional value instead of a mere
boolean:

.Parser library: cond_map
[source,ml]
----
let cond_map msg f x corr rest =
  ct_bind (take rest) (function
    | EndOfStream, rest' ->
      if BoundedSet.is_full corr then
        ct_return []
      else
        ct_return [ x, change_at corr EndOfStream msg, rest' ]
    | Item (tok, _pos) as item, rest' ->
      (match f tok with
       | Some v -> ct_return [ v, corr, rest' ]
       | None   ->
        if BoundedSet.is_full corr then
          ct_return []
        else
          ct_return [ x, change_at corr item  msg, rest' ]))
----

.Parser signatures
[source,ml]
----
val cond_map : string -> ('a -> 'b option) -> 'b -> ('a, 'b, 'c) t
----

from which we can write a simpler +cond+ parser:

// TODO: a way in portia to say "replaces 'Parser library: the cond parser'"
// TODO (alt): A definition starting with same name a one that already
//             exist, followed by a coma and something should replace it in
//             the output instead of been appended to it.
.Parser library: the cond parser, revisited
[source,ml]
----
let cond msg f =
  cond_map msg (fun c -> if f c then Some c else None)
----


It is possible to build many simpler and more convenient parsers on top of
+cond+, such as +item+ which expects a specific token in the input, and
+range+ which expect anything in the given character range:

.Parser library: the item parser
[source,ml]
----
let item ?(what="item") x =
  cond ("missing "^what) ((=) x) x

let range a b msg =
  cond msg (fun c -> c >= a && c <= b) a
----

.Parser signatures
[source,ml]
----
val item : ?what:string -> 'a -> ('a, 'a, 'c) t
val range : 'a -> 'a -> string -> ('a, 'a, 'c) t
----

.tests
[source,ml]
----
"item canonical success" >:: (
  fun _ctx ->
    assert_same_results Char.print
      ['g', no_corr, List.tl rest]
      (item 'g' no_corr rest)
) ;
"item canonical failure" >:: (
  fun _ctx ->
    assert_same_results Char.print
      []
      (item 'X' no_corr rest)
) ;
"item fails at EOF" >:: (
  fun _ctx ->
    assert_same_results Char.print
      []
      (item 'g' no_corr no_input)
) ;
"item error exploration" >:: (
  fun _ctx ->
    assert_same_results Char.print
      ['X', correction_at (Item ('g', 0)) "missing item", List.tl rest]
      (item 'X' corr rest)
) ;
----

== Modifying parsers result

Before going too far we need to introduce functions to alter a parser result (equivalent of map, fold, filter...) and come up with a convenient syntax for those since they are going to be used prevalently.

.Parser library: applying a function to all results of a parser
[source,ml]
----
let map p f corr rest =
  ct_bind (p corr rest) (fun results ->
    List.map (fun (x, corr, rest) -> f x, corr, rest) results |>
    ct_return)
----

The order of parameters is important so that +map p f+ is itself a parser.
An infix operator makes it even more convenient:

.Parser library: infix operator for map
[source,ml]
----
let (>>:) = map
----

.Parser signatures
[source,ml]
----
val map :   ('a, 'b, 'c) t -> ('b -> 'd) -> ('a, 'd, 'c) t
val (>>:) : ('a, 'b, 'c) t -> ('b -> 'd) -> ('a, 'd, 'c) t
----

== Combinators

The first combinators to consider are the succession of two given parsers,
the alternative of two parsers, and the pipe of one parser result into the
input of another one.

Notice that since we are now merely combining parsers we do not have to care
about error correction any more: only the base parsers need to pretend
succeeding when they fail.

The more general way to build a combinator for the succession of two parsers is
to take the first parser +p1+ and a function +f+ which, given the output of
+p1+, will return a parser +p2+ to apply to the remaining of the input stream.
Let's call this combinator +bind+ (by analogy with the type of the +bind+
operation in the monad ``design pattern''). The values of +bind p1 f+ are the
values of +p2+, +p1+ intermediary values being only meaningful to build +p2+.

.Parser library: bind
[source,ml]
----
let bind p1 f corr rest =
  ct_bind (p1 corr rest) (fun results ->
    (* for each possible result of p1, try to continue parsing with p2 *)
    List.fold_left (fun res_list' (x1, corr1, unp1) ->
        let p2 = f x1 in
        ct_bind (p2 corr1 unp1) (function
          | [] -> res_list'
          | res_list2 ->
            ct_bind (res_list') (fun lst ->
              ct_return (List.rev_append res_list2 lst)))
      ) (ct_return []) results)
----

With the usual infix operator:

.Parser library: infix operator for bind
[source,ml]
----
let (>>=) = bind
----

.Parser signatures
[source,ml]
----
val bind :  ('a, 'b, 'c) t -> ('b -> ('a, 'd, 'c) t) -> ('a, 'd, 'c) t
val (>>=) : ('a, 'b, 'c) t -> ('b -> ('a, 'd, 'c) t) -> ('a, 'd, 'c) t
----

Given this +bind+ combinator, the concatenation of two given parsers +p1+ and
+p2+ can be easily written as:

.Parser library: succession of two parsers
[source,ml]
----
let cons p1 p2 =
  p1 >>= (fun x1 -> p2 >>: fun x2 -> x1,x2)
----

Here, we want the final result set to be the product of each result of +p1+
with all following results of +p2+.

This parser being used to connect successive parsers we'd rather have a
shorter infix alternative for +cons+:

.Parser library: infix operator for cons
[source,ml]
----
let (++) p1 p2 = cons p1 p2
----

.Parser signatures
[source,ml]
----
val cons : ('a, 'b, 'c) t -> ('a, 'd, 'c) t -> ('a, 'b * 'd, 'c) t
val (++) : ('a, 'b, 'c) t -> ('a, 'd, 'c) t -> ('a, 'b * 'd, 'c) t
----

Also, we will often discard the result of one parser. For instance when
parsing delimiters the only information is that the parser succeeds (there is
a delimiter) but there is no value to attach to that success. Also when using
the +check+ parser, which purpose is really not its return value. So here are
three variants of +cons+: one that ignores the result of +p1+, one that
ignores the result of +p2+, and one that ignore both (returning +()+):

.Parser library: other convenient infix operators
[source,ml]
----
let (+-) p1 p2 = p1 ++ p2 >>: fst
let (-+) p1 p2 = p1 ++ p2 >>: snd
let (--) p1 p2 = p1 ++ p2 >>: fun _ -> ()
----

.Parser signatures
[source,ml]
----
val (+-) : ('a, 'b, 'c) t -> ('a, 'd, 'c) t -> ('a, 'b, 'c) t
val (-+) : ('a, 'b, 'c) t -> ('a, 'd, 'c) t -> ('a, 'd, 'c) t
val (--) : ('a, 'b, 'c) t -> ('a, 'd, 'c) t -> ('a, unit, 'c) t
----

Now let's test that we can indeed sequence parsers:

.tests
[source,ml]
----
"Can parse a sequence" >:: (
  fun _ctx ->
    let ab = stream_of_string "ab" in
    assert_same_results (Tuple2.print Char.print Char.print)
      [('a', 'b'), no_corr, []]
      ((item 'a' ++ item 'b') no_corr ab) ;
    assert_same_results Char.print
      ['a', no_corr, []]
      ((item 'a' +- item 'b') no_corr ab)
) ;
----

The second most useful combinator is the alternative:

.Parser library: alternative
[source,ml]
----
let oneof p1 p2 corr rest =
  ct_bind (p1 corr rest) (fun res_list1 ->
    ct_bind (p2 corr rest) (fun res_list2 ->
      ct_return (List.rev_append res_list1 res_list2)))

let (|||) = oneof
----

.Parser signatures
[source,ml]
----
val oneof : ('a, 'b, 'c) t -> ('a, 'b, 'c) t -> ('a, 'b, 'c) t
val (|||) : ('a, 'b, 'c) t -> ('a, 'b, 'c) t -> ('a, 'b, 'c) t
----

Notice that results are really sets not list, so the order in which the
alternatives are listed does not matter.  Notice also that this is not an
exclusive alternative: if both +p1+ and +p2+ can parse then both will
contribute a result to the result set. As discussed in the beginning we do
not enforce that if +p1+ succeeds then +p2+ must fail nor the other way
around. If this is wanted though then it is easy enough to write:

.Parser library: exclusive alternative
[source,ml]
----
let either p1 p2 =
  (check "??1" (nay p2) -+ p1) ||| (check "??2" (nay p1) -+ p2)

let (|/|) = either
----

.Parser signatures
[source,ml]
----
val either : ('a, 'b, 'c) t -> ('a, 'b, 'c) t -> ('a, 'b, 'c) t
val (|/|) :  ('a, 'b, 'c) t -> ('a, 'b, 'c) t -> ('a, 'b, 'c) t
----

With sequences and alternatives we can start writing some interesting tests:

.tests
[source,ml]
----
"any: 'a' or 'b' but not 'z'" >:: (
  fun _ctx ->
    let a_or_b = item 'a' ||| item 'b' in
    assert_same_results Char.print
      ['a', no_corr, []]
      (a_or_b no_corr (stream_of_string "a")) ;
    assert_same_results Char.print
      ['b', no_corr, []]
      (a_or_b no_corr (stream_of_string "b")) ;
    assert_same_results Char.print
      []
      (a_or_b no_corr (stream_of_string "z")) ;
    assert_same_results Char.print
      ['a', correction_at (Item ('z', 0)) "missing item", [] ;
       'b', correction_at (Item ('z', 0)) "missing item", []]
      (a_or_b corr (stream_of_string "z"))
) ;
----

== Repeating parsers

Binding several parsers already gives us a way to harvest several values from
the input stream but many times what is needed is to repeat the same parser an
unspecified number of times.

Before that, a special case of repetition will prove very useful: having zero
or one occurrence of +p+:

.Parser library: zero or one
[source,ml]
----
let optional ~def p = p ||| return def
let optional_greedy ~def p = (p +- check "??3" (nay p)) ||| return def
----

The +optional_greedy+ above is to avoid terminating a parser on an optional
parser, which results in two possible result systematically, which the caller
will not necessarily reduce if the next parser starts with the same optional
parser. Indeed,

[source,ML]
----
optional p ++ optional p
----

would yield 2 equivalent results.

.Parser signatures
[source,ml]
----
val optional : def:'b -> ('a, 'b, 'c) t -> ('a, 'b, 'c) t
val optional_greedy : def:'b -> ('a, 'b, 'c) t -> ('a, 'b, 'c) t
----

The +repeat+ combinator is a swiss-army knife for all variants of repetitions,
requiring a parser +p+ to succeed from +min+ to +max+ times consecutively, with
an optional additional parser +sep+ for a separator in between +p+ occurrences.
It returns a list of all values returned by the successive +p+.

But allowing +min+ to be +0+ (and making it the default value) we expect to
cut down on the many +optional (repeat p)+ that we would have otherwise.

TODO: make sure that all parsers used in there behave properly using tests first!

.Parser library: repetition of a parser
[source,ml]
----
let rec repeat ~sep ?(min=0) ?max p corr rest =
  if max = Some 0 then (
    if min = 0 then return [] corr rest
    else fail corr rest
  ) else (
    let pred_ma = match max with None -> None
                               | Some m -> Some (m-1) in
    match min with 0 ->
      (* we may stop here or continue *)
      (optional ~def:[] (repeat ~sep ~min:1 ?max p)) corr rest
                | 1 ->
      (* at least one more, everything else optional *)
      ((p ++ optional ~def:[]
                 (sep -+ (repeat ~sep ~min:1 ?max:pred_ma p))) >>:
        fun (x, xs) -> x::xs) corr rest
                | _ ->
      (* above that, repetition is mandatory *)
      ((p +- sep ++ repeat ~sep ~min:(min-1) ?max:pred_ma p) >>:
        fun (x, xs) -> x::xs) corr rest
  )
----

Notice there are two conditions that terminate the recursion: +max+ reaching
+0+ (no more occurrences permitted) or, when +min > 0+, a failure of +p+.

Notice also that repeat builds a whole list before sending it to the next
stage.  We'd like to get away with this list which, most often than not will be
changed into something else value by value. A lazy list (or a BatEnum) would
likely be preferable here (as in other places).

.Parser signatures
[source,ml]
----
val repeat :
  sep:('a, 'd, 'c) t -> ?min:int -> ?max:int -> ('a, 'b, 'c) t -> ('a, 'b list, 'c) t
----



We'd like to get away with the mandatory +sep+ parameter using a default value
of +return ()+ but that would prevent OCaml compiler to infer that since +sep+
result is consistently discarded any result type would be as good.  Simpler
example of this using the _REPL_:

----
# let f ?sep x = x ;;
val f : ?sep:'a -> 'b -> 'b = <fun>
# let f ?(sep=42) x = x;;
val f : ?sep:int -> 'a -> 'a = <fun>
----

Therefore we merely provide this short do-nothing constant parser to be used
when there is no separator:

.Parser library: none
[source,ml]
----
let none corr = return () corr
----

You may be surprised by this notation, either because you were expecting +let
none corr rest = return () corr rest+ or the shorter +let none = return ()+.
Refer to the appendix about type generalization if that is the case (TODO: link).

.Parser signatures
[source,ml]
----
val none : ('a, unit, 'c) t
----

We can easily define the greedy version of +repeat+ (that is, a version that
swallows as many +p+ occurrences as present in the input stream) using check:

.(erroneous) greedy repetition
[source,ml]
----
let repeat_greedy ~sep ?min ?max p =
  repeat ~sep ?min ?max p +- check "extraneous" (nay (sep -+ p))
----

...which unfortunately fails for +min=0+ because of the separator.  We have to
be more cautious not to allow an input stream starting with +p+ before
returning +[]+:

.Parser library: greedy repetition
[source,ml]
----
let rec repeat_greedy ~sep ?min ?max p =
  match min with
  | None | Some 0 ->
    repeat_greedy ~sep ~min:1 ?max p ||| (check "extraneous" (nay p) >>: fun () -> [])
  | min ->
    repeat ~sep ?min ?max p +- (check "extraneous" (nay (sep -+ p)) >>: fun _ -> [])
----

.Parser signatures
[source,ml]
----
val repeat_greedy :
  sep:('a, 'd, 'c) t -> ?min:int -> ?max:int -> ('a, 'b, 'c) t -> ('a, 'b list, 'c) t
----

.tests
[source,ml]
----
"repetition: canonical successes" >:: (
  fun _ctx ->
    let assert_ok ?(greedy=false) ~sep ?min ?max rest exp =
      assert_same_results (List.print Char.print)
        exp
        ((if greedy then repeat_greedy else repeat) ~sep ?min ?max (item 'a') no_corr rest) in
    let test_with_sep sep sep_len rest =
      let drop n = List.drop (1 + (n-1)*(1+sep_len)) rest in
      assert_ok ~sep rest
        [['a';'a';'a'], no_corr, drop 3 ;
         ['a';'a'],     no_corr, drop 2 ;
         ['a'],         no_corr, drop 1 ;
         [],            no_corr, rest] ;
      (* Same with min=2 *)
      assert_ok ~sep ~min:2 rest
        [['a';'a';'a'], no_corr, drop 3 ;
         ['a';'a'],     no_corr, drop 2] ;
      (* Testing max=2 *)
      assert_ok ~sep ~max:2 rest
        [['a';'a'],     no_corr, drop 2;
         ['a'],         no_corr, drop 1;
         [],            no_corr, rest] ;
      (* Now with min and max *)
      assert_ok ~sep ~min:1 ~max:2 rest
        [['a';'a'],     no_corr, drop 2 ;
         ['a'],         no_corr, drop 1] ;
      (* min = max *)
      assert_ok ~sep ~min:2 ~max:2 rest
        [['a';'a'],     no_corr, drop 2] in
    let aaab = stream_of_string "aaab"
    and a_a_a_b = stream_of_string "a_a_a_b"
    and _a_a_a_b = stream_of_string "_a_a_a_b" in
    test_with_sep none       0 aaab ;
    test_with_sep (item '_') 1 a_a_a_b ;
    assert_ok ~greedy:true ~sep:none aaab
      [['a';'a';'a'], no_corr, List.drop 3 aaab] ;
    assert_ok ~greedy:true ~sep:(item '_') a_a_a_b
      [['a';'a';'a'], no_corr, List.drop 5 a_a_a_b] ;
    (* Do not allow a separator at start *)
    assert_ok ~greedy:true ~sep:(item '_') _a_a_a_b
      [[], no_corr, _a_a_a_b]
) ;
"repetition: simplest failure" >:: (
  fun _ctx ->
    assert_same_results (List.print Char.print)
      []
      (repeat ~sep:none ~min:1 (item 'a') no_corr (stream_of_string "zaab")) ;
    assert_same_results (List.print Char.print)
      []
      (repeat_greedy ~sep:none ~min:1 (item 'a') no_corr (stream_of_string "zaab"))
) ;
"repetition: missing separator" >:: (
  fun _ctx ->
    assert_same_results (List.print Char.print)
      []
      (repeat ~sep:(item '-') ~min:3 (item 'a') no_corr (stream_of_string "a-aab")) ;
    assert_same_results (List.print Char.print)
      []
      (repeat_greedy ~sep:(item '-') ~min:3 (item 'a') no_corr (stream_of_string "a-aab"))
) ;
"repetition: below min" >:: (
  fun _ctx ->
    assert_same_results (List.print Char.print)
      []
      (repeat ~sep:none ~min:2 (item 'a') no_corr (stream_of_string "baab")) ;
    assert_same_results (List.print Char.print)
      []
      (repeat_greedy ~sep:none ~min:2 (item 'a') no_corr (stream_of_string "baab"))
) ;
----

Some variants of +repeat+ can now be defined:

.Parser library: repeat variants
[source,ml]
----
let several ~sep = repeat ~sep ~min:1
let several_greedy ~sep = repeat_greedy ~sep ~min:1
let times ~sep n = repeat ~sep ~min:n ~max:n
----

.Parser signatures
[source,ml]
----
val several : sep:('a, 'd, 'c) t -> ?max:int -> ('a, 'b, 'c) t -> ('a, 'b list, 'c) t
val several_greedy : sep:('a, 'z, 'c) t -> ?max:int -> ('a, 'b, 'c) t -> ('a, 'b list, 'c) t
val times : sep:('a, 'z, 'c) t -> int -> ('a, 'b, 'c) t -> ('a, 'b list, 'c) t
----

With all these new combinators, more interesting tests can be devised:

.tests
[source,ml]
----
"Several combinators bound together" >:: (
  fun _ctx ->
    let p = decimal_digit >>= (fun c ->
      let i = Char.code c - Char.code '0' in
      assert_bool "not a digit" (i >= 0 && i <= 9) ;
      (* match a sequence of i zeros *)
      times ~sep:none i (item '0')) in
    let rest1 = stream_of_string "105"
    and rest2 = stream_of_string "100"
    and rest3 = stream_of_string "30005"
    and rest4 = stream_of_string "3005" in
    assert_same_results (List.print Char.print)
      [['0'], no_corr, List.drop 2 rest1]
      (p no_corr rest1) ;
    assert_same_results (List.print Char.print)
      [['0'], no_corr, List.drop 2 rest2]
      (p no_corr rest2) ;
    assert_same_results (List.print Char.print)
      [['0';'0';'0'], no_corr, List.drop 4 rest3]
      (p no_corr rest3) ;
    assert_same_results (List.print Char.print)
      []
      (p no_corr rest4)
) ;
----

.Parser library: trivial parsers and utilities
[source,ml]
----
let string s =
  let rec loop i =
    if i >= String.length s then return ()
    else (
      (item ~what:s s.[i]) -- (loop (i+1))
    ) in
  loop 0

let replace x _ = x

let anything corr rest =
  ct_bind (take rest) (function
    | EndOfStream, _rest' -> ct_return []
    | Item (tok, _pos), rest' -> ct_return [ tok, corr, rest' ])
----

Notice that +anything+ cannot really fail.

.Parser signatures
[source,ml]
----
val string : string -> (char, unit, 'c) t
val replace : 'a -> 'b -> 'a
val anything : ('a, 'a, 'c) t
----

== Miscellaneous

=== Type generalization

Let's get back to why we haven't defined +none+ simply as +let none = return
()+, using automatic curryfication (TODO: link) to lighten the syntax:

----
# let none = return ();;
             ^^^^^^^^^
Error: The type of this expression, ('_a, unit, '_b) t,
       contains type variables that cannot be generalized
----

This is actually a limitation of OCaml compiler. Here is what's happening:
normally, in an expression like +let name = expr+, +expr+ will be typed first,
leading in this case where +expr+ is actually +return ()+ to the type +(`_a,
unit, '_b) t+ (where +'_a+ and +'_b+ are ``weak types'' (refer to the
definition of +return+: it's merely a function of 3 parameters returning a list
of the triplet of these 3 parameters). Once +expr+ is typed, OCaml follow this
rule: if +expr+ is a function (as in +function ... ->+), a constant or an
identifier then generalize the weak types into universal types (the more
familiar +'a+, +'b+ etc). If +erpx+ is anything fancier, though, such as a
partial application as is the case here, then do not generalize.

If instead we had +let name params... = expr+ then, given it's syntactic sugar
for +let name = function ... -> expr+ then the ``weak types'' would have been
generalized.

So we have to make this looks more like a function, by making explicit at least
one parameter (a process famously known under the tickling name
``eta-expansion'').

This feels arbitrary because it is ; apparently this is one of the minor
disadvantage of a typing rules that has plenty of other advantages such as
making something already quite complex much simpler. See the OCaml FAQ (TODO:
link) for more details.

This is unfortunately going to hit us a lot when defining parser combinators
because we'd like to get away with the many meaningless and repetitive
parameters which presence just obfuscate the intent of the code. C'est la vie.

=== BoundedSet

We still have to provide an implementation for our set of fixed maximum size.
The simplest implementation is that of a list with a current size:

.BoundedSet.ml: type
[source,ml]
----
type 'a t =
  { size : int ;
    max_size : int ;
    items : 'a list }
----

With the trivial constructor:

.BoundedSet.ml: constructor
[source,ml]
----
let make max_size =
  { size = 0 ; max_size ; items = [] }
----

And the only two operations we've met so far:

.BoundedSet.ml: operations
[source,ml]
----
let is_full t = t.size >= t.max_size

let add t x =
  { t with size = t.size + 1 ;
           items = x::t.items }
----

=== Printers

If there is something annoying about OCaml it's the lack of default printers
for types. +Batteries+ provides +dump+ but it is oblivious to constructors so
the result is not pretty. So let's write our own printers.

It would be best to provide formatters instead of mere printers to benefit
from automatic typesetting but unfortunately +Batteries+ support for those is
minimal so it's better to forget about formatters to cut down on typing.

.BoundedSet.ml: printer
[source,ml]
----
open Batteries

let print print_value fmt t =
  List.print print_value fmt t.items
----

.Corrections.ml: printers
[source,ml]
----
open Batteries

let print_position print_coord fmt = function
  | EndOfStream -> String.print fmt "end of input"
  | Item c -> print_coord fmt c

let print_correction print_coord fmt (pos, msg) =
  Printf.fprintf fmt "%s at %a"
    msg
    (print_position print_coord) pos

let print_corrections print_coord fmt corr =
  BoundedSet.print (print_correction print_coord) fmt corr
----

.Parser library: printers
[source,ml]
----
let print_result print_coord print_output fmt (x, corr, _rest) =
  Printf.fprintf fmt "(output=%a,corrections=%a)"
    print_output x
    (print_corrections print_coord) corr
----

.Parser signatures
[source,ml]
----
val print_result : ('o BatInnerIO.output -> ('a * 'c) -> unit) ->
                   ('o BatInnerIO.output -> 'b -> unit) ->
                   'o BatInnerIO.output ->
                   ('a, 'b, 'c) result -> unit
----

== Parsing usual things

It might come handy to have some ready made parsers for common things such
as words, numbers, etc... We will regroup those in a +Usual+ module, again
offering a functor:

.Usual.ml: Parsers for usual things
[source,ml]
----
open Batteries
module Make (P : Parsers.S) =
struct
  open P
  (* ...usual parsers... *)
end
----

...that we will test along with the +Parsers+ module:

.other tested modules
[source,ml]
----
module SimpleUsual = Usual.Make (P)
open SimpleUsual
----

=== Numbers

We will try to follow the most common conventions for parsing numbers.
Notice that a simple base 10 integer number must start with a non 0
(otherwise it's octal).  We make no exception for the single digit '0' which
we will parse as octal.

.usual parsers: integers
[source,ml]
----
type integer = Int.t (* or maybe not? *)

let decimal_digit corr =
  range '0' '9' "digit" corr

let non_zero_decimal_digit corr =
  range '1' '9' "non-zero digit" corr

let num_of_char c =
  let cc = Char.code c in
  if cc >= Char.code '0' && cc <= Char.code '9' then
    cc - Char.code '0'
  else if cc >= Char.code 'a' && cc <= Char.code 'f' then
    cc - Char.code 'a' + 10
  else if cc >= Char.code 'A' && cc <= Char.code 'F' then
    cc - Char.code 'A' + 10
  else invalid_arg "c"

let underscore corr = item '_' corr

let unsigned_decimal_number corr rest =
  let digits corr = several ~sep:none decimal_digit corr in
  (non_zero_decimal_digit +-
   optional ~def:' ' underscore ++
   optional ~def:[] (several ~sep:underscore digits) >>:
   fun (first, next) ->
   List.fold_left (fun c digits ->
     List.fold_left (fun c digit ->
       c * 10 + num_of_char digit) c digits) 0 ([first]::next)) corr rest

let signed neg p corr =
  (p                                     |||
   item ~what:"sign" '+' -+ p            |||
   (item ~what:"sign" '-' -+ p >>: neg)) corr

let decimal_number corr =
  signed Int.neg unsigned_decimal_number corr
----

We have made +num_of_char+ accept hexadecimal digits in foresight.

Octals, hexadecimal and binary numbers are then build similarly: a mandatory
prefix, and some digits interleaved with underscores. Notice that only the
prefix is mandatory and '0x' for instance is a valid immediate (representing
zero of course), as in perl.

.usual parsers: non decimal integers
[source,ml]
----
let non_decimal_integer base prefix digit corr =
  let digits corr = several ~sep:none digit corr in
  (prefix -+ repeat ~sep:underscore digits >>:
     List.fold_left (fun c digits ->
       List.fold_left (fun c digit ->
         c * base + num_of_char digit) c digits) 0) corr

let octal_digit corr =
  range '0' '7' "octal digit" corr

let octal_number corr =
  (non_decimal_integer 8 (item ~what:"0" '0') octal_digit |>
   signed Int.neg) corr

let hexadecimal_digit corr =
  cond "hexadecimal digit" (fun c ->
    (c >= '0' && c <= '9') ||
    (c >= 'a' && c <= 'f') ||
    (c >= 'A' && c <= 'F')) '1' corr

let non_decimal_integer_prefix x =
  item ~what:"0" '0' --
  cond "integer prefix" (fun c -> Char.lowercase c = x) x

let hexadecimal_number corr =
  let prefix = non_decimal_integer_prefix 'x' in
  (non_decimal_integer 16 prefix hexadecimal_digit |>
   signed Int.neg) corr

let binary_digit corr =
  range '0' '1' "bit" corr

let binary_number corr =
  let prefix = non_decimal_integer_prefix 'b' in
  (non_decimal_integer 2 prefix binary_digit |>
   signed Int.neg) corr
----

Finally, this parser can parse all kinds of integers seen so far:

.usual parsers: any integer
[source,ml]
----
let integer corr =
  (decimal_number     |||
   octal_number       |||
   hexadecimal_number |||
   binary_number) corr
----

.tests
[source,ml]
----
"integer immediate" >:: (
  fun _ctx ->
    [ "4", 4 ;
      "42", 42 ;
      "12345", 12345 ;
      "4_294_967_296", 4_294_967_296 ;
      "042", 0o42 ;
      "0x42", 0x42 ;
      "0X42", 0x42 ;
      "0xff", 0xff ;
      "0b10", 0b10 ;
      "0x", 0 ;
      "0x4_2", 0x4_2 ;
      "-4", -4 ;
      "+4", 4 ;
      "-042", -0o42 ;
      "+042", 0o42 ;
      "-0x42", -0x42 ;
      "-0b10", -0b10 ] |>
    List.iter (fun (input, output) ->
      assert_same_results Int.print
        [output, no_corr, []]
        ((integer +- eof "char") no_corr (stream_of_string input)))
) ;
"not decimal number immediate" >:: (
  fun _ctx ->
    [ "0_" ; "0X_" ; "_123" ; "123_" ; "12__34" ; "_" ; "_0x123" ;
      "-0_" ; "-_42" ] |>
    List.iter (fun input ->
      assert_same_results Int.print
        []
        ((integer +- eof "char") no_corr (stream_of_string input)))
) ;
----

The syntax for floating point numbers is more _perly_.  Indeed, in additional
to the usual decimal and scientific notations, Perl allows hexadecimal floating
point, with a power of two as the exponent (and a "p" instead of an "e" to
introduce the exponent, for obvious reason).

Also, notice that you can omit either the integer or the fractional part but
not both.

.usual parsers: floating point
[source,ml]
----
let fractional_part inv_base digit =
  let digits corr = several ~sep:none digit corr in
  several ~sep:underscore digits >>: fun digits ->
    List.fold_left (fun c_scale digits ->
      List.fold_left (fun (c, scale) digit ->
        let n = num_of_char digit |> float_of_int in
        c +. n *. scale, scale *. inv_base) c_scale digits) (0., inv_base) digits |>
    fst

let unsigned_decimal_fractional corr =
  let dot corr = item ~what:"fractional dot" '.' corr in
  ((unsigned_decimal_number +- dot ++ fractional_part 0.1 decimal_digit) |||
   (return 0 +- dot ++ fractional_part 0.1 decimal_digit)                |||
   (unsigned_decimal_number +- dot ++ return 0.) >>:
       fun (n, p) -> float_of_int n +. p
  ) corr

let decimal_fractional corr =
  signed Float.neg unsigned_decimal_fractional corr

let decimal_scientific corr =
  ((decimal_fractional |||
    (decimal_number >>: float_of_int)) +-
   cond "exponent delimiter" (fun c -> c = 'e' || c = 'E') 'e' ++
   decimal_number >>: fun (m, e) ->
     m *. Float.pow 10. (float_of_int e) (* FIXME *)
   ) corr

let floating_point corr =
  (decimal_fractional |||
   decimal_scientific) corr
----

.tests
[source,ml]
----
"floating point notation" >:: (
  fun _ctx ->
    [ "3.14", 3.14 ;
      "-3.14", -3.14 ;
      "314e2", 31400. ;
      "314e-2", 3.14 ;
      ".1", 0.1 ;
      "1.", 1.0 ] |>
    List.iter (fun (input, output) ->
      assert_same_results Float.print
        [output, no_corr, []]
        ((floating_point +- eof "char") no_corr (stream_of_string input)))
) ;
----

And finally the function that reads any immediate number:

.usual parsers: any number
[source,ml]
----
type number = Int of integer
            | Float of float
let number corr =
  ((integer        >>: fun x -> Int x) |||
   (floating_point >>: fun x -> Float x)) corr
----

=== Character types

It is common to check for various classes of characters: blanks, numerics,
alphanumerics, newlines...

.usual parsers: blanks
[source,ml]
----
let blank corr =
  cond "space" (fun c -> c = ' ' || c = '\t') ' ' corr

let newline corr =
  (optional ~def:'\r' (item ~what:"\\r" '\r') -+
   item ~what:"end of line" '\n') corr

let whitespace corr =
  repeat_greedy ~sep:none (blank ||| newline) corr

let opt_whitespace corr =
  optional_greedy ~def:[] whitespace corr
----

Notice we read greedily the whitespaces because we want to avoid a +whitespace
-- whitespace+ ambiguity. +optional_greedy+ is there for the same reason.

.usual parsers: other character classes
[source,ml]
----
let lowercase corr = range 'a' 'z' "lowercase" corr
let uppercase corr = range 'A' 'Z' "uppercase" corr
let letter corr = (lowercase ||| uppercase) corr
let alphanum corr = (letter ||| decimal_digit) corr
----

=== Operations

Operators are another frequent occurrence. Of course they tend more than
character classes to depend on the problem at hand, but it's still useful to
discuss them here if only to demonstrate how to deal with recursive rules.

Indeed, the straightforward way to define a parser for operations would rely on
left recursion, which a combinatory parser can't do. Instead, we will have to
_force_ _progress_ by defining a chain of terms and subterms in order of
precedence.

The principle of such a chain is to replace a left recursing definition such
as:

[source,ml]
----
let term corr = (term +- any_binary_op ++ term) corr
----

with:

[source,ml]
----
let term1 corr = ((term2 +- low_precedence_op ++ term2) ||| term2) corr
let term2 corr = ((term3 +- higher_precedence_op ++ term3) ||| term3) corr
(* etc... *)
----

Allowing recursion only after some input have been consumed:

[source,ml]
----
let rec this_is_ok corr =
  (item '{' -+ this_is_ok ++ item '}') corr
let rec this_is_infinite_recursion corr =
  (this_is_infinite_recursion ++ anything_else) corr
let rec this_is_still_infinite_recursion corr =
  (check "a check does not consume" some_check ++
   this_is_still_infinite_recursion) corr
----

Now this chain will always parse left side first. If +1 + 2 * 3+ will properly
be parsed as +1 + (2 * 3)+ (because the parse would fail if +term1+ consumed
only +1 + 2+), the simple +3 - 2 - 1+ would be erroneously parsed as +3 - (2 -
1)+ instead of +(3 - 2) - 1+. To help with left associative operators, we need
to group operators of same precedence and associativity and use a +repeat+
parser, which associativity we are free to choose.

Here is a +binary_ops_reducer+ parser that takes a parser for binary operators
of same associativity and precedence (here called +op+), and a parser for terms
(called +term+), and returns either the left or right associativity parser. It
is expected that the +term+ parser has higher precedence. It bear some
resemblance with +repeat+ but does not discard the output of the separator
(here: the operation) and build as a last stage the final result out of the
list of partial results, with the expected associativity.  This situation
occurs often enough in practice that it's worth having a generic solution in
the parser combinator library. It is made generic enough by the use of another
parameter, the +reduce+ function, that combines two terms and an operator
results into a value of the same type as returned by term. Notice that this may
force the user of this +binary_ops_reducer+ function to `lift` the sub-term
parser in order to return a singleton term instead (if the sub-terms and terms
do not share a common type).

.Parser library: binary operations with selected associativity
[source,ml]
----
let binary_ops_reducer ?(right_associative=false) ~op ~term ~sep ~reduce corr =
  (term +- sep ++ repeat ~sep (op +- sep ++ term) >>:
   fun (fst, lst) -> (* lst is a list of (op result * term result) *)
     let rec loop_lst last_term = function
       | [] -> last_term
       | (op, next_term)::rest ->
         if right_associative then
           reduce last_term op (loop_lst next_term rest)
         else
           loop_lst (reduce last_term op next_term) rest
         in
     loop_lst fst lst) corr
----

.Parser signatures
[source,ml]
----
val binary_ops_reducer :
  ?right_associative:bool ->
  op:('a, 'o, 'c) t ->
  term:('a, 'b, 'c) t ->
  sep:('a, unit, 'c) t ->
  reduce:('b -> 'o -> 'b -> 'b) ->
  ('a, 'b, 'c) t
----

Let's see it in action:

.tests
[source,ml]
----
"binary_ops_reducer" >:: (
  fun _ctx ->
    let term corr = (decimal_digit >>: fun c -> Term c) corr in
    let op corr = item '+' corr in
    let reduce t1 _op t2 = Op (t1, t2) in
    [ "1+2",
        Op (Term '1', Term '2'),
        Op (Term '1', Term '2') ;
      "1+2+3",
        Op (Op (Term '1', Term '2'), Term '3'),
        Op (Term '1', Op (Term '2', Term '3')) ;
      "1+2+3+4",
        Op (Op (Op (Term '1', Term '2'), Term '3'), Term '4'),
        Op (Term '1', Op (Term '2', Op (Term '3', Term '4'))) ] |>
    List.iter (fun (input_str, e1, e2) ->
      (* e1 is the expected result for left associative parsing and
         e2 for right associative parsing. *)
      let input = stream_of_string input_str in
      assert_same_results ~msg:"left assoc." binary_ops_reducer_test_result_print
        [e1, no_corr, []]
        ((binary_ops_reducer ~op ~term ~sep:none ~reduce ~right_associative:false
          +- eof "term") no_corr input) ;
      assert_same_results ~msg:"right assoc." binary_ops_reducer_test_result_print
        [e2, no_corr, []]
        ((binary_ops_reducer ~op ~term ~sep:none ~reduce ~right_associative:true
          +- eof "term") no_corr input))
) ;
----

with type +binary_ops_reducer_test_result+ defined globally, as required by OCaml:

.other global functions or types for testing
[source,ml]
----
type binary_ops_reducer_test_result =
    Term of Char.t
  | Op of (binary_ops_reducer_test_result *
           binary_ops_reducer_test_result)

let rec binary_ops_reducer_test_result_print fmt = function
  | Term c ->
     Printf.fprintf fmt "%c" c
  | Op (r1, r2) ->
     Printf.fprintf fmt "(%a+%a)"
       binary_ops_reducer_test_result_print r1
       binary_ops_reducer_test_result_print r2
----

Let's also test the handling of precedence with a small calculator:

.tests
[source,ml]
----
"precedence and associativity" >:: (
  fun _ctx ->
    let value corr = (decimal_digit >>: num_of_char) corr in
    let reduce t1 op t2 = match op with
      | '+' -> t1+t2 | '-' -> t1-t2
      | '*' -> t1*t2 | '/' -> t1/t2
      | '^' -> int_of_float((float_of_int t1)**(float_of_int t2))
      | _ -> assert false in
    let rec left_assoc_low_prec corr =
      binary_ops_reducer ~op:(item '+' ||| item '-')
                         ~term:left_assoc_high_prec
                         ~sep:none ~reduce corr
    and left_assoc_high_prec corr =
      binary_ops_reducer ~op:(item '*' ||| item '/')
                         ~term:right_assoc_higher_prec
                         ~sep:none ~reduce corr
    and right_assoc_higher_prec corr =
      binary_ops_reducer ~op:(item '^')
                         ~right_associative:true
                         ~term:left_assoc_highest_prec
                         ~sep:none ~reduce corr
    and left_assoc_highest_prec corr =
      (value |||
       item '(' -+ left_assoc_low_prec +- item ')') corr in
    [ "0",            0 ;
      "1+2",          3 ;
      "1+2+3",        6 ;
      "1+2+3+4",     10 ;
      "5-1",          4 ;
      "5-4-1",        0 ;
      "(5-4)-1",      0 ;
      "5-(4-1)",      2 ;
      "4^3^2",   262144 ;
      "4^(3^2)", 262144 ;
      "(4^3)^2",   4096 ;
      "3*2+1",        7 ;
      "1+3*2",        7 ;
      "(1+3)*2",      8 ;
      "8/2/2",        2 ] |>
    List.iter (fun (input_str, exp) ->
      let input = stream_of_string input_str in
      assert_same_results Int.print
        [exp, no_corr, []]
        ((left_assoc_low_prec +- eof "term") no_corr input))
) ;
----

Hopefully this example shed some confidence on parsing operators with any precedence and
associativity despite using parser combinators.


